{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "\n",
    "# Specific plotting style to make the plots look even nicer\n",
    "plt.style.use(\"bmh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading processed data from the numpy file\n",
    "dat = np.load(\"finaldata.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data for training and testing; data is also reshaped\n",
    "window = 36 # The number of hours used as input and output; in our case samples of 36 hours\n",
    "shift = 1*24 # Shifting the period of time used for training and testing; this is due to a larger 'window' resulting in not having data at the start of the train data\n",
    "index_start = 12 + shift # Index of when training set should start; this is 1 September 2015 12:00; this is also shifted by one day\n",
    "train_end_date = \"01_02_2021_12\" # End date for training set\n",
    "test_start_date = \"10_02_2021_12\" # Start date for test set\n",
    "train_end_ind = dat[:,0].tolist().index(train_end_date) +shift # Dates translated to index in the data\n",
    "test_start_ind = dat[:,0].tolist().index(test_start_date) +shift\n",
    "n_test = 7 # Number of days to predict\n",
    "\n",
    "input_length = 24 # Timesteps; we are moving with 24 hours when going to a new sample\n",
    "xtrain_length = int((train_end_ind-index_start)/input_length) # Total number of samples\n",
    "\n",
    "# Defining the train data\n",
    "X_train_temp = dat[index_start-(window-24):train_end_ind,1:].astype('float64')\n",
    "y_train_temp = dat[index_start+input_length:train_end_ind+input_length+(window-24),1].astype('float64')\n",
    "\n",
    "# Since we use the previous 36 hours to predict 36 hours ahead, each sample will have 12 hours repeated from the\n",
    "# previous sample (36-24); thus the data is reshaped in such a way that the 36 hours window is moved with steps of 24 hours\n",
    "# through the data and then stacked\n",
    "X_train = np.array([X_train_temp[0:window]])\n",
    "y_train = y_train_temp[0:window]\n",
    "for i in range(1,xtrain_length):\n",
    "    X_train = np.vstack((X_train,np.array([X_train_temp[i*24:i*24+window]])))\n",
    "    y_train = np.vstack((y_train,y_train_temp[i*24:i*24+window]))\n",
    "y_train = np.expand_dims(y_train, axis=2)\n",
    "\n",
    "# Defining the test data\n",
    "X_test_temp = dat[test_start_ind-(window-24):test_start_ind + n_test*input_length,1:].astype('float64')\n",
    "y_test_temp = dat[test_start_ind+input_length:test_start_ind + n_test*input_length + input_length+(window-24),1].astype('float64')\n",
    "\n",
    "# Same as for the training data, where the 36 hours window is stacked for steps of 24 hours\n",
    "X_test = np.array([X_test_temp[0:window]])\n",
    "y_test = y_test_temp[0:window]\n",
    "for i in range(1,n_test):\n",
    "    X_test = np.vstack((X_test,np.array([X_test_temp[i*24:i*24+window]])))\n",
    "    y_test = np.vstack((y_test,y_test_temp[i*24:i*24+window]))\n",
    "y_test = np.expand_dims(y_test, axis=2)\n",
    "\n",
    "print(\"Input train shape\",X_train.shape)\n",
    "print(\"Output train shape\",y_train.shape)\n",
    "\n",
    "print(\"Input test shape\",X_test.shape)\n",
    "print(\"Output test shape\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which returns the configuration of the network\n",
    "def config_layers(X_train, NNH_1, NNH_2,NNH_3, y_train):\n",
    "    num_input_units = np.shape(X_train)[1] # Number of inputs\n",
    "    NNH_1 = NNH_1 # Number of hidden neurons for layer 1\n",
    "    NNH_2 = NNH_2 # Number of hidden neurons for layer 2\n",
    "    NNH_2 = NNH_3 # Number of hidden neurons for layer 3\n",
    "    num_logits = np.shape(y_train)[1] # Number of logits\n",
    "    return num_input_units, NNH_1, NNH_2,NNH_3, num_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the quantile losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function defining the quantile loss\n",
    "def quantile_loss(q, y_p, y):\n",
    "        e = y_p-y\n",
    "        return tf.keras.backend.mean(tf.keras.backend.maximum(q*e, (q-1)*e))\n",
    "\n",
    "# Defining the different quantiles and putting them together\n",
    "loss_q_1 = lambda y,f: quantile_loss(0.01,y,f)\n",
    "loss_q_5 = lambda y,f: quantile_loss(0.05,y,f)\n",
    "loss_q_10 = lambda y,f: quantile_loss(0.10,y,f)\n",
    "loss_q_25 = lambda y,f: quantile_loss(0.25,y,f)\n",
    "loss_q_50 = lambda y,f: quantile_loss(0.50,y,f)\n",
    "loss_q_75 = lambda y,f: quantile_loss(0.75,y,f)\n",
    "loss_q_90 = lambda y,f: quantile_loss(0.90,y,f)\n",
    "loss_q_95 = lambda y,f: quantile_loss(0.95,y,f)\n",
    "loss_q_99 = lambda y,f: quantile_loss(0.99,y,f)\n",
    "\n",
    "quantile_loss_array = [loss_q_1, loss_q_5, loss_q_10, loss_q_25, loss_q_50, loss_q_75, loss_q_90, loss_q_95, loss_q_99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the configuration for the different layers of the network\n",
    "num_input_units, NNH_1, NNH_2,NNH_3, num_logits = config_layers(X_train,19,20,19,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hyperparameters of the network\n",
    "learning_rate = 0.001\n",
    "epochs = 300\n",
    "validation_split = 0.25\n",
    "batch_size = 125\n",
    "\n",
    "# Defining the optimizer\n",
    "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "# Defining the layers; LSTM layers are all bidirectional\n",
    "input_layer = keras.layers.Input(shape=(num_input_units, 7))\n",
    "lstm1_layer = keras.layers.Bidirectional(keras.layers.LSTM(NNH_1), input_shape=(num_input_units, 7))(input_layer)\n",
    "repeat_layer = keras.layers.RepeatVector(num_logits)(lstm1_layer) # Repeats its input 36 times to get back to original 3D shape\n",
    "lstm2_layer = keras.layers.Bidirectional(keras.layers.LSTM(NNH_2, return_sequences=True, activation='softplus'))(repeat_layer)\n",
    "lstm3_layer = keras.layers.Bidirectional(keras.layers.LSTM(NNH_3, return_sequences=True, activation='softplus'))(lstm2_layer)\n",
    "# Dropout layer for regularization\n",
    "dropout_layer = keras.layers.Dropout(0.20)(lstm3_layer)\n",
    "# Each quantile uses the previously defined layers to make sure that they use the same network to get to the loss\n",
    "quantile_output_q_1 = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='softplus'))(dropout_layer)\n",
    "quantile_output_q_5 = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='softplus'))(dropout_layer)\n",
    "quantile_output_q_10 = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='softplus'))(dropout_layer)\n",
    "quantile_output_q_25 = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='softplus'))(dropout_layer)\n",
    "quantile_output_q_50 = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='softplus'))(dropout_layer)\n",
    "quantile_output_q_75 = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='softplus'))(dropout_layer)\n",
    "quantile_output_q_90 = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='softplus'))(dropout_layer)\n",
    "quantile_output_q_95 = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='softplus'))(dropout_layer)\n",
    "quantile_output_q_99 = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='softplus'))(dropout_layer)\n",
    "\n",
    "# Defining the model\n",
    "model = keras.models.Model(inputs=input_layer, outputs=[quantile_output_q_1,quantile_output_q_5,\n",
    "                                                        quantile_output_q_10,quantile_output_q_25,\n",
    "                                                        quantile_output_q_50,quantile_output_q_75,\n",
    "                                                        quantile_output_q_90,quantile_output_q_95,quantile_output_q_99])\n",
    "\n",
    "# Defining the compiler with the loss function and the optimizer\n",
    "model.compile(loss=quantile_loss_array , optimizer=optimizer)\n",
    "\n",
    "# Callback for keeping the best iteration of the model during training\n",
    "checkpoint_path = \"learned_weights/weights.best.hdf5\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                     monitor='val_loss',\n",
    "                                                     mode='auto',\n",
    "                                                     save_best_only=True,\n",
    "                                                     save_weights_only=False,\n",
    "                                                     save_freq='epoch',\n",
    "                                                     verbose=1)\n",
    "\n",
    "# Fitting the model\n",
    "history = model.fit(X_train, [y_train]*9, validation_split=validation_split,\\\n",
    "                    epochs=epochs, batch_size=batch_size, callbacks = [cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the model loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizes the losses for the epochs\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line of code below to save the trained model\n",
    "\n",
    "# model.save('my_awesome_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line of code below to load a previously trained model\n",
    "# !!!Make sure to run the first 3 cells of this file before loading the model!!!\n",
    "\n",
    "# model = keras.models.load_model('final_model_epoch300_lr0_0005_vs0_25_bs125', compile = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediciting the output on our test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.asarray(y_pred) # Converting to a Numpy array\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the RMSE error and the quantile mean loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the RMSE error between the predicted quantile 50 and the actual data\n",
    "rmse_error = np.sqrt(np.mean((y_pred[4,:,12:36].flatten()-y_test[:,12:36].flatten())**2))\n",
    "\n",
    "#Calculating the mean quantile loss:\n",
    "quantiles = [0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99]\n",
    "qloss = 0 \n",
    "for i in range(len(y_pred)):\n",
    "    e = y_pred[i,:,12:36].flatten()-y_test[:,12:36].flatten()\n",
    "    currentqloss = np.mean(np.maximum(quantiles[i]*(e), (quantiles[i]-1)*e))\n",
    "    qloss = qloss + currentqloss\n",
    "    print(\"Average quantile loss for quantile\",quantiles[i], \":\",currentqloss)\n",
    "\n",
    "quantile_mean_loss = qloss/len(quantiles)\n",
    "print('RMSE error =', rmse_error)\n",
    "print(\"Quantile mean loss =\",quantile_mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the four regions of the quantiles, the actual value and the predicted value (quantile 50)\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.fill_between(range(24*7),y_pred[0,:,12:36].flatten(),y_pred[-1,:,12:36].flatten(), color = \"#999999\", label = \"quantile 1-99\")\n",
    "plt.fill_between(range(24*7),y_pred[1,:,12:36].flatten(),y_pred[-2,:,12:36].flatten(), color = \"#808080\", label = \"quantile 5-95\")\n",
    "plt.fill_between(range(24*7),y_pred[2,:,12:36].flatten(),y_pred[-3,:,12:36].flatten(), color = \"#666666\", label = \"quantile 10-90\")\n",
    "plt.fill_between(range(24*7),y_pred[3,:,12:36].flatten(),y_pred[-4,:,12:36].flatten(), color = \"#4d4d4d\", label = \"quantile 25-75\")\n",
    "plt.plot(y_test[:,12:36].flatten(),color=\"red\", label = \"actual value\")\n",
    "plt.plot(y_pred[4,:,12:36].flatten(),color=\"blue\", label = \"predicted value\")\n",
    "\n",
    "# Plotting vertical lines to seperate the days\n",
    "plt.axvline(x=24-1, color=\"black\", linestyle=\"--\")\n",
    "plt.axvline(x=24*2-1, color=\"black\", linestyle=\"--\")\n",
    "plt.axvline(x=24*3-1, color=\"black\", linestyle=\"--\")\n",
    "plt.axvline(x=24*4-1, color=\"black\", linestyle=\"--\")\n",
    "plt.axvline(x=24*5-1, color=\"black\", linestyle=\"--\")\n",
    "plt.axvline(x=24*6-1, color=\"black\", linestyle=\"--\")\n",
    "plt.axvline(x=24*7-1, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "plt.xticks([11,35,59,83,107,131,155],['Day 1', 'Day 2','Day 3', 'Day 4', 'Day 5', 'Day 6', 'Day 7'])\n",
    "plt.xlim(0,24*7-1)\n",
    "plt.ylabel(\"Day-ahead prices [Euros]\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
